{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MGT-448 Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.double\n",
    "device = torch.device(\"cuda:0\")\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "tolerance = 1e-5\n",
    "def rel_error(x, y):\n",
    "    return torch.max(torch.abs(x - y) / (torch.maximum(1e-8, torch.abs(x) + torch.abs(y))))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "iteration = 20\n",
    "reg = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 28])\n",
      "torch.Size([325, 28])\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "data_train = np.loadtxt(open('./logisticRegressionData/training.csv', \"r\"), delimiter=\",\", skiprows=0)\n",
    "data_train = torch.from_numpy(data_train.astype(np.float64)).to(device)\n",
    "print(data_train.size())\n",
    "data_test = np.loadtxt(open('./logisticRegressionData/testing.csv', \"r\"), delimiter=\",\", skiprows=0)\n",
    "data_test = torch.from_numpy(data_test.astype(np.float64)).to(device)\n",
    "print(data_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train[:, 1:]\n",
    "y_train = data_train[:, 0].view(data_train.size(0), 1)\n",
    "x_test = data_test[:, 1:]\n",
    "y_test = data_test[:, 0].view(data_test.size(0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "(a) In logistic regression, the classifier maps the vector-form input to classes. In binomial LR, if we represent the classes with binary numbers 0 or 1, and denote the ith input data as $x_i$ and output class as $y_i$, one typical example is $y_i = \\text{sigmoid} (\\theta^T x_i)$ where $x_i\\in\\mathbb{R}^d, \\theta\\in\\mathbb{R}^{d}, y_i\\in\\mathbb{R}$. \n",
    "\n",
    "(b) Write $h_{\\theta}(x_i) = \\text{sigmoid}(\\theta x_i)$. The likelihood function is \n",
    "$$L(\\theta) = \\Pi_{i=1}^n(h_{\\theta}(x_i))^{y_i}(1-h_{\\theta}(x_i))^{1-y_i}$$ \n",
    "We mainly use the log-likelihood function, which is \n",
    "$$l(\\theta) = \\Sigma_{i=1}^n y_i\\ln(h_{\\theta}(x_i))+(1-y_i)\\ln(1-h_{\\theta}(x_i))$$ \n",
    "Suppose we have $n$ input and output pairs. Combine all inputs to a single matrix $X$, each row of which is one input vector, then $X\\in\\mathbb{R}^{n\\times d}$. Combine all output data as vector $y$. Then the mapping becomes $y = \\text{sigmoid} (X \\theta)$. Then the vectorized log-likelihood function is \n",
    "$$l(\\theta) = y^T\\ln(h_{\\theta}(X))+(1-y)^T\\ln(1-h_{\\theta}(X))$$\n",
    "The cost function would be $J(\\theta) = - \\frac{1}{n} l(\\theta) + \\frac{1}{2}\\lambda\\left\\|\\theta\\right\\|^2$. For the case of without regularization, the gradient of the loss function is \n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta} = \\frac{1}{n}X^T (h_{\\theta}(X)-y)$$ and its hessian is \n",
    "$$H(\\theta) = \\frac{1}{n}X^T \\text{diag} \\left\\{(1-h_{\\theta}(X))h_{\\theta}(X)\\right\\}X$$\n",
    "By Newton's Method, the iteration for $\\theta$ is \n",
    "$$\\theta = \\theta - (H(\\theta))^{-1}\\frac{\\partial J(\\theta)}{\\partial \\theta}$$\n",
    "Our algorithm starts with initializing $\\theta$ as zero vector and stops when $J(\\theta)$ is smaller than a predifined value.\n",
    "\n",
    "(c) Classification error is given by $1-\\frac{1}{n} (y \\text{ AND } (\\text{sigmoid}(\\theta X)>0.5))$.\n",
    "\n",
    "(d)  1) the dependent variables should be binary. 2) the observations should be mutually independent and the inputs/independent variable are not highly correlated. 3) the dependent and independent variables are linearly correlated. 4) relatively large sample size.\n",
    "\n",
    "## 2\n",
    "In the update of $\\theta$ in Newton's Method, we use \n",
    "$$\\theta = \\theta - (H(\\theta))^{-1}\\frac{\\partial J(\\theta)}{\\partial \\theta} + \\lambda \\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, D_out = x_train.size(0), x_train.size(1), 1\n",
    "\n",
    "y_train_binary = torch.fmod(y_train, 2)\n",
    "# print(y_train_binary)\n",
    "# theta_2 = torch.randn(1, D_out, device=device, dtype=dtype)\n",
    "\n",
    "def log_likelyhood_bin_cal(hx, y):\n",
    "    return torch.t(y).mm(torch.log(hx)) + torch.t(1-y).mm(torch.log(1-hx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newtons's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  0.6931471805599452\n",
      "Loss is:  0.24310494300010352\n",
      "Loss is:  0.11022913121958734\n",
      "Loss is:  0.051880942051840746\n",
      "Loss is:  0.02438109485788122\n",
      "Loss is:  0.011814006920286325\n",
      "Loss is:  0.007395360643426095\n",
      "Loss is:  0.007213385523406105\n",
      "Loss is:  nan\n",
      "\n",
      "Parameters theta_1 is:\n",
      " [[ 1.057e+00]\n",
      " [ 2.273e+00]\n",
      " [-2.084e+00]\n",
      " [ 3.072e-03]\n",
      " [-7.010e+00]\n",
      " [ 6.582e+00]\n",
      " [-7.115e-01]\n",
      " [ 9.534e-01]\n",
      " [-3.766e+00]\n",
      " [ 1.439e+00]\n",
      " [ 1.329e+00]\n",
      " [-1.598e+00]\n",
      " [ 4.141e-01]\n",
      " [-6.146e+00]\n",
      " [ 6.085e+00]\n",
      " [-6.501e-01]\n",
      " [ 6.027e-01]\n",
      " [-1.540e+00]\n",
      " [ 1.144e+00]\n",
      " [ 2.042e-01]\n",
      " [-6.010e-01]\n",
      " [ 1.599e-01]\n",
      " [-3.249e+00]\n",
      " [ 3.820e+00]\n",
      " [-1.089e+00]\n",
      " [-6.903e-01]\n",
      " [ 6.860e-01]]\n"
     ]
    }
   ],
   "source": [
    "i, loss, step_size = 0, 1.0, 1.0\n",
    "theta_1 = torch.zeros(D_in, D_out, device=device, dtype=dtype)\n",
    "while i < iteration and loss > tolerance:\n",
    "    i = i+1\n",
    "    # forward pass\n",
    "    scores = x_train.mm(theta_1)\n",
    "    # computing loss\n",
    "    h_1 = sigmoid(scores)\n",
    "    h_0 = 1 - h_1\n",
    "    log_likelyhood = log_likelyhood_bin_cal(h_1, y_train_binary)\n",
    "    loss = -log_likelyhood/N + 0.5*reg*torch.sum(theta_1*theta_1)\n",
    "    print(\"Loss is: \", loss.item())\n",
    "    \n",
    "    # optimization\n",
    "    grad_loss = torch.t(x_train).mm(h_1-y_train_binary)\n",
    "    # print(grad_loss.size())\n",
    "    temp_diag = torch.diag(torch.flatten(h_0*h_1 + tolerance))\n",
    "    # print(temp_diag.size(), x_train.size())\n",
    "    hessian_loss = torch.t(x_train).mm(temp_diag).mm(x_train)\n",
    "    dtheta_1 = torch.inverse(hessian_loss).mm(grad_loss) \n",
    "    theta_1 -= step_size * dtheta_1 + reg*theta_1\n",
    "    \n",
    "print(\"\\nParameters theta_1 is:\\n\", theta_1.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.6861538461538461\n"
     ]
    }
   ],
   "source": [
    "# check accuracy for test data\n",
    "y_test_binary = torch.fmod(y_test, 2)\n",
    "y_test_cal = x_test.mm(theta_1)\n",
    "sigmoid_y = 1/(1+torch.exp(-y_test_cal)) > 0.5\n",
    "# print(sigmoid_y)\n",
    "accuracy = torch.true_divide(torch.sum(torch.abs(y_test_binary * sigmoid_y.int())), y_test_binary.size(0))\n",
    "print(\"Accuracy is \", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking line search for Newton's Method in binomial case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  0.6931471805599452\n",
      "Criterion function value is:  tensor([[False]], device='cuda:0')\n",
      "\n",
      "Parameters theta_1 is:\n",
      " [[-0.122]\n",
      " [ 0.066]\n",
      " [ 0.03 ]\n",
      " [-0.022]\n",
      " [-0.517]\n",
      " [ 0.382]\n",
      " [ 0.176]\n",
      " [ 0.436]\n",
      " [-0.433]\n",
      " [-0.043]\n",
      " [-0.045]\n",
      " [ 0.098]\n",
      " [-0.018]\n",
      " [-0.486]\n",
      " [ 0.445]\n",
      " [ 0.176]\n",
      " [ 0.33 ]\n",
      " [-0.267]\n",
      " [-0.029]\n",
      " [-0.013]\n",
      " [ 0.108]\n",
      " [ 0.008]\n",
      " [-0.189]\n",
      " [ 0.185]\n",
      " [ 0.063]\n",
      " [-0.332]\n",
      " [ 0.302]]\n"
     ]
    }
   ],
   "source": [
    "i, loss, step_size = 0, 1.0, 1.0\n",
    "theta_1 = torch.zeros(D_in, D_out, device=device, dtype=dtype)\n",
    "\n",
    "alpha, rho, c = 1.0, 0.5, 0.5\n",
    "criterion_v = torch.ones(1, 1, device=device, dtype=dtype)\n",
    "\n",
    "while criterion_v and loss > tolerance and i < iteration:\n",
    "    i = i+1\n",
    "    # forward pass\n",
    "    scores = x_train.mm(theta_1)\n",
    "    # computing loss\n",
    "    h_1 = sigmoid(scores)\n",
    "    h_0 = 1 - h_1\n",
    "    log_likelyhood = log_likelyhood_bin_cal(h_1, y_train_binary)\n",
    "    loss = -log_likelyhood/N + 0.5*reg*torch.sum(theta_1*theta_1)\n",
    "    print(\"Loss is: \", loss.item())\n",
    "    \n",
    "    # optimization\n",
    "    grad_loss = torch.t(x_train).mm(h_1-y_train_binary)\n",
    "    # print(grad_loss.size())\n",
    "    temp_diag = torch.diag(torch.flatten(h_0*h_1 + tolerance))\n",
    "    # print(temp_diag.size(), x_train.size())\n",
    "    hessian_loss = torch.t(x_train).mm(temp_diag).mm(x_train)\n",
    "    dtheta_1 = torch.inverse(hessian_loss).mm(grad_loss) \n",
    "    theta_1 -= step_size * dtheta_1 + reg*theta_1\n",
    "    \n",
    "    # backtracking linear search related\n",
    "    pk = -dtheta_1\n",
    "    alpha = rho*alpha\n",
    "    criterion = (-log_likelyhood_bin_cal(sigmoid(x_train.mm(theta_1+alpha*pk)), y_train_binary)/N \\\n",
    "                + log_likelyhood_bin_cal(h_1, y_train_binary)/N \\\n",
    "                - c*alpha*torch.t(grad_loss/N).mm(pk)) > 0\n",
    "    criterion_v = torch.sum(criterion.int())\n",
    "    print(\"Criterion function value is: \", criterion)\n",
    "    \n",
    "print(\"\\nParameters theta_1 is:\\n\", theta_1.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.676923076923077\n"
     ]
    }
   ],
   "source": [
    "# check accuracy for test data\n",
    "y_test_binary = torch.fmod(y_test, 2)\n",
    "y_test_cal = x_test.mm(theta_1)\n",
    "sigmoid_y = 1/(1+torch.exp(-y_test_cal)) > 0.5\n",
    "# print(sigmoid_y)\n",
    "accuracy = torch.true_divide(torch.sum(torch.abs(y_test_binary * sigmoid_y.int())), y_test_binary.size(0))\n",
    "print(\"Accuracy is \", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  0.6931471805599452\n",
      "Loss is:  nan\n",
      "\n",
      "Parameters theta_1 is:\n",
      " [[-7.615]\n",
      " [-4.415]\n",
      " [-6.437]\n",
      " [-9.837]\n",
      " [-5.34 ]\n",
      " [-8.709]\n",
      " [-8.001]\n",
      " [-3.585]\n",
      " [-6.566]\n",
      " [-0.545]\n",
      " [ 0.781]\n",
      " [-0.443]\n",
      " [ 2.914]\n",
      " [ 3.531]\n",
      " [ 4.412]\n",
      " [ 2.539]\n",
      " [ 1.466]\n",
      " [ 2.585]\n",
      " [ 1.387]\n",
      " [ 0.037]\n",
      " [ 0.274]\n",
      " [ 1.466]\n",
      " [ 0.062]\n",
      " [ 0.302]\n",
      " [ 1.242]\n",
      " [ 0.054]\n",
      " [ 0.25 ]]\n"
     ]
    }
   ],
   "source": [
    "i, loss, step_size = 0, 1.0, 1.0\n",
    "decay_rate = 0.5\n",
    "theta_1 = torch.zeros(D_in, D_out, device=device, dtype=dtype)\n",
    "while i < iteration and (loss > tolerance or torch.isinf(loss)):\n",
    "    i = i+1\n",
    "    step_size = step_size * decay_rate\n",
    "    # forward pass\n",
    "    scores = x_train.mm(theta_1)\n",
    "    # computing loss\n",
    "    h_1 =sigmoid(scores)\n",
    "    h_0 = 1 - h_1\n",
    "    log_likelyhood = log_likelyhood_bin_cal(h_1, y_train_binary)\n",
    "    loss = -log_likelyhood/N + 0.5*reg*torch.sum(theta_1*theta_1)\n",
    "    print(\"Loss is: \", loss.item())\n",
    "    \n",
    "    # optimization\n",
    "    grad_loss = torch.t(x_train).mm(h_1-y_train_binary)\n",
    "    dtheta_1 = grad_loss/N \n",
    "    theta_1 -= step_size * dtheta_1 + reg*theta_1\n",
    "    \n",
    "print(\"\\nParameters theta_1 is:\\n\", theta_1.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.0\n"
     ]
    }
   ],
   "source": [
    "# check accuracy for test data\n",
    "y_test_binary = torch.fmod(y_test, 2)\n",
    "y_test_cal = x_test.mm(theta_1)\n",
    "sigmoid_y = 1/(1+torch.exp(-y_test_cal)) > 0.5\n",
    "# print(sigmoid_y)\n",
    "accuracy = torch.true_divide(torch.sum(torch.abs(y_test_binary * sigmoid_y.int())), y_test_binary.size(0))\n",
    "print(\"Accuracy is \", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch size of  1  is:  0.6931471805599453\n",
      "Loss for batch size of  1  is:  nan\n",
      "Accuracy for batch size of  1  is  0.0 \n",
      "\n",
      "Loss for batch size of  16  is:  0.6931471805599453\n",
      "Loss for batch size of  16  is:  63.746611187724454\n",
      "Loss for batch size of  16  is:  nan\n",
      "Accuracy for batch size of  16  is  0.7415384615384616 \n",
      "\n",
      "Loss for batch size of  32  is:  0.6931471805599453\n",
      "Loss for batch size of  32  is:  nan\n",
      "Accuracy for batch size of  32  is  0.0 \n",
      "\n",
      "Loss for batch size of  198  is:  0.6931471805599451\n",
      "Loss for batch size of  198  is:  3.989342096473571\n",
      "Loss for batch size of  198  is:  20.609948201080222\n",
      "Loss for batch size of  198  is:  4.775562067711667\n",
      "Loss for batch size of  198  is:  1.5170529384595168\n",
      "Loss for batch size of  198  is:  0.4736798617308025\n",
      "Loss for batch size of  198  is:  0.3577950947049763\n",
      "Loss for batch size of  198  is:  0.3565033691348357\n",
      "Loss for batch size of  198  is:  0.35604808863576726\n",
      "Loss for batch size of  198  is:  0.3558405515645653\n",
      "Loss for batch size of  198  is:  0.3557458582495657\n",
      "Loss for batch size of  198  is:  0.3557059560426358\n",
      "Loss for batch size of  198  is:  0.35569312727212976\n",
      "Loss for batch size of  198  is:  0.3556937633759889\n",
      "Loss for batch size of  198  is:  0.3557011151324394\n",
      "Loss for batch size of  198  is:  0.3557118208785451\n",
      "Loss for batch size of  198  is:  0.3557242029337879\n",
      "Loss for batch size of  198  is:  0.35573742324075\n",
      "Loss for batch size of  198  is:  0.3557510629686805\n",
      "Loss for batch size of  198  is:  0.35576491275339456\n",
      "Accuracy for batch size of  198  is  0.6584615384615384 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "step_size, decay_rate = 1.0, 0.5\n",
    "\n",
    "batch_sizes = [1, 16, 32, N]\n",
    "theta_s = torch.zeros(D_in, D_out, len(batch_sizes), device=device, dtype=dtype)\n",
    "\n",
    "y_test_binary = torch.fmod(y_test, 2)\n",
    "\n",
    "for k in range(len(batch_sizes)):\n",
    "    batch_size = batch_sizes[k]\n",
    "    loss = 1.0\n",
    "    theta_1 = torch.zeros(D_in, D_out, device=device, dtype=dtype)\n",
    "    perm_seq = torch.randperm(N)\n",
    "    x_train_perm, y_train_binary_perm = x_train[perm_seq, :], y_train_binary[perm_seq, :]\n",
    "    \n",
    "    for j in range(int(np.ceil(N/batch_size))):\n",
    "        # print(\"j is \", j)\n",
    "        start_ind = batch_size*j\n",
    "        end_ind = np.minimum(N, start_ind+batch_size)\n",
    "        # print(\"end_ind is \",  end_ind)\n",
    "        x_batch = x_train_perm[start_ind:end_ind]\n",
    "        y_batch = y_train_binary_perm[start_ind:end_ind]\n",
    "        N_batch = end_ind - start_ind\n",
    "        \n",
    "        i = 0\n",
    "        while i < iteration and (loss > tolerance or torch.isinf(loss)):\n",
    "            i = i+1\n",
    "            step_size = step_size * decay_rate\n",
    "            # forward pass\n",
    "            scores = x_batch.mm(theta_1)\n",
    "            # computing loss\n",
    "            h_1 = sigmoid(scores)\n",
    "            h_0 = 1 - h_1\n",
    "            log_likelyhood = log_likelyhood_bin_cal(h_1, y_batch)\n",
    "            loss = -log_likelyhood/N_batch + 0.5*reg*torch.sum(theta_1*theta_1)\n",
    "            print(\"Loss for batch size of \", batch_size, \" is: \", loss.item())\n",
    "            # optimization\n",
    "            grad_loss = torch.t(x_batch).mm(h_1-y_batch)\n",
    "            dtheta_1 = grad_loss/N_batch \n",
    "            theta_1 -= step_size * dtheta_1 + reg*theta_1\n",
    "    \n",
    "    theta_s[:, :, k] = theta_1\n",
    "    # print(\"\\nParameters theta_1 is:\\n\", theta_1.data.cpu().numpy())\n",
    "    y_test_cal = x_test.mm(theta_1)\n",
    "    sigmoid_y = sigmoid(y_test_cal) > 0.5\n",
    "    # print(sigmoid_y)\n",
    "    accuracy = torch.true_divide(torch.sum(torch.abs(y_test_binary * sigmoid_y.int())), y_test_binary.size(0))\n",
    "    print(\"Accuracy for batch size of \", batch_size, \" is \", accuracy.item(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "N, D_in, D_out = x_train.size(0), x_train.size(1), num_classes\n",
    "\n",
    "y_train_expand = torch.zeros(y_train.size(0), num_classes, device=device, dtype=dtype)\n",
    "# print(y_train_expand.size())\n",
    "for i in range(num_classes):\n",
    "    temp = y_train == i+1\n",
    "    y_train_expand[:, i] = torch.flatten(temp.int())\n",
    "\n",
    "def log_likelyhood_mul_cal(hx, y):\n",
    "    temp = torch.t(y).mm(torch.log(hx)) + torch.t(1-y).mm(torch.log(1-hx))\n",
    "    return torch.diag(temp).view(num_classes, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is  [[0.693 0.693 0.693 0.693]]\n",
      "Loss is  [[0.286 0.201 0.216 0.179]]\n",
      "Loss is  [[0.175 0.09  0.108 0.08 ]]\n",
      "Loss is  [[0.106 0.043 0.057 0.039]]\n",
      "Loss is  [[0.066 0.02  0.029 0.02 ]]\n",
      "Loss is  [[0.034 0.01  0.015 0.01 ]]\n",
      "Loss is  [[0.015 0.006 0.009 0.007]]\n",
      "Loss is  [[0.007 0.005 0.009   nan]]\n",
      "\n",
      "Parameters theta_2 is:\n",
      " [[ 1.147 -0.278 -0.757 -0.69 ]\n",
      " [-1.96  -2.543 -1.328 -0.947]\n",
      " [-1.082  1.433  0.903  1.148]\n",
      " [-0.116 -0.197 -0.243 -0.778]\n",
      " [-0.708  6.223  0.516 -7.925]\n",
      " [ 0.803 -2.69   3.032  1.651]\n",
      " [-0.672 -0.155  0.836  1.806]\n",
      " [-0.064 -1.635  7.541  1.308]\n",
      " [ 1.176  2.103 -7.393 -0.224]\n",
      " [ 0.639 -0.745 -0.016 -0.417]\n",
      " [-1.648 -1.006 -1.494 -0.561]\n",
      " [ 0.364  0.268  0.598  0.818]\n",
      " [ 0.601 -0.462 -0.251 -0.971]\n",
      " [-1.464  5.625  0.135 -8.616]\n",
      " [ 0.789 -2.332  3.184  1.99 ]\n",
      " [-0.42  -0.389  0.519  1.818]\n",
      " [ 0.213 -0.98   6.359  1.529]\n",
      " [ 1.388  1.982 -4.876 -1.632]\n",
      " [ 0.31  -0.263 -0.517  0.505]\n",
      " [-1.147 -0.117 -0.397  2.351]\n",
      " [-0.081 -0.032  2.351 -2.2  ]\n",
      " [ 0.213 -0.219 -0.217 -1.291]\n",
      " [-1.895  3.09  -1.066 -4.717]\n",
      " [ 2.522 -1.214  1.29   3.31 ]\n",
      " [-0.485 -0.239  0.83   1.353]\n",
      " [-0.958 -0.364 -2.136  2.833]\n",
      " [ 2.222  0.419  0.747 -2.961]]\n"
     ]
    }
   ],
   "source": [
    "i, step_size = 0, 1.0\n",
    "loss = torch.ones(num_classes, device=device, dtype=dtype)\n",
    "theta_2 = torch.zeros(D_in, D_out, device=device, dtype=dtype)\n",
    "\n",
    "while i < iteration and torch.max(loss) > tolerance:\n",
    "    i = i+1\n",
    "    # forward pass\n",
    "    scores = x_train.mm(theta_2)\n",
    "    # computing loss\n",
    "    h_1 = 1 / (1+torch.exp(-scores))\n",
    "    h_0 = 1 - h_1\n",
    "    log_likelyhood = log_likelyhood_mul_cal(h_1, y_train_expand)\n",
    "    loss = -log_likelyhood/N + 0.5*reg*torch.diag(torch.t(theta_2).mm(theta_2)).view(num_classes, 1)\n",
    "    print(\"Loss is \", loss.data.cpu().numpy().T)\n",
    "    \n",
    "    # optimization\n",
    "    grad_loss = torch.t(x_train).mm(h_1-y_train_expand)\n",
    "    hessian_loss = torch.zeros(D_in, D_in, num_classes, device=device, dtype=dtype)\n",
    "    dtheta_2 = torch.zeros(theta_2.size(), device=device, dtype=dtype)\n",
    "    for i in range(num_classes):\n",
    "        temp_diag = torch.diag(torch.flatten(h_0[:, i]*h_1[:, i]+tolerance))\n",
    "        hessian_loss[:, :, i] = torch.t(x_train).mm(temp_diag).mm(x_train)\n",
    "        temp = torch.inverse(hessian_loss[:, :, i]).mm(grad_loss[:, i].view(D_in, 1))\n",
    "        dtheta_2[:, i] = torch.flatten(torch.inverse(hessian_loss[:, :, i]).mm(grad_loss[:, i].view(D_in, 1)))\n",
    "    theta_2 -= step_size * dtheta_2 + reg*theta_2    \n",
    "    \n",
    "print(\"\\nParameters theta_2 is:\\n\", theta_2.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.7876923076923077\n"
     ]
    }
   ],
   "source": [
    "# check accuracy for test data\n",
    "y_test_expand = torch.zeros(y_test.size(0), num_classes, device=device, dtype=dtype)\n",
    "# print(y_test_expand.size())\n",
    "for i in range(num_classes):\n",
    "    temp = y_test == i+1\n",
    "    y_test_expand[:, i] = torch.flatten(temp.int())\n",
    "\n",
    "y_test_cal = x_test.mm(theta_2)\n",
    "sigmoid_y = 1/(1+torch.exp(-y_test_cal)) > 0.5\n",
    "# print(sigmoid_y)\n",
    "accuracy = torch.true_divide(torch.sum(torch.abs(y_test_expand * sigmoid_y.int())), y_test_expand.size(0))\n",
    "print(\"Accuracy is \", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtrack line search for Newton's Method in multinomial case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is  [[0.693 0.693 0.693 0.693]]\n",
      "Criterion function value is:  0\n",
      "\n",
      "Parameters theta_2 is:\n",
      " [[-0.055  0.114 -0.073  0.002]\n",
      " [-0.029 -0.027  0.096 -0.038]\n",
      " [ 0.13  -0.071 -0.101  0.04 ]\n",
      " [ 0.101 -0.041 -0.125  0.061]\n",
      " [ 0.266  0.357 -0.824  0.121]\n",
      " [-0.249 -0.092  0.658 -0.263]\n",
      " [-0.012 -0.115  0.194 -0.053]\n",
      " [ 0.057 -0.289  0.441 -0.086]\n",
      " [-0.069  0.236 -0.439  0.122]\n",
      " [-0.043  0.025 -0.006  0.012]\n",
      " [-0.169  0.168  0.125 -0.123]\n",
      " [ 0.251 -0.201 -0.154  0.102]\n",
      " [ 0.093 -0.028 -0.113  0.044]\n",
      " [ 0.261  0.365 -0.786  0.081]\n",
      " [-0.119 -0.172  0.589 -0.247]\n",
      " [ 0.073 -0.148  0.111 -0.021]\n",
      " [ 0.192 -0.278  0.2    0.01 ]\n",
      " [-0.218  0.253 -0.125 -0.062]\n",
      " [-0.047  0.012  0.016  0.016]\n",
      " [-0.032  0.035  0.027 -0.015]\n",
      " [ 0.075 -0.071  0.027 -0.043]\n",
      " [ 0.052 -0.016 -0.046  0.007]\n",
      " [ 0.038  0.264 -0.238 -0.085]\n",
      " [ 0.067 -0.193  0.128  0.019]\n",
      " [ 0.017 -0.051  0.049 -0.01 ]\n",
      " [-0.433  0.219  0.114  0.125]\n",
      " [ 0.398 -0.21  -0.118 -0.115]]\n"
     ]
    }
   ],
   "source": [
    "i, step_size = 0, 1.0\n",
    "loss = torch.ones(num_classes, device=device, dtype=dtype)\n",
    "theta_2 = torch.zeros(D_in, D_out, device=device, dtype=dtype)\n",
    "\n",
    "alpha, rho, c = 1.0, 0.5, 0.5\n",
    "criterion_v = torch.ones(1, 1, device=device, dtype=dtype)\n",
    "\n",
    "while i < iteration and torch.max(loss) > tolerance and criterion_v:\n",
    "    i = i+1\n",
    "    # forward pass\n",
    "    scores = x_train.mm(theta_2)\n",
    "    # computing loss\n",
    "    h_1 = sigmoid(scores)\n",
    "    h_0 = 1 - h_1\n",
    "    log_likelyhood = log_likelyhood_mul_cal(h_1, y_train_expand)\n",
    "    loss = -log_likelyhood/N + 0.5*reg*torch.diag(torch.t(theta_2).mm(theta_2)).view(num_classes, 1)\n",
    "    print(\"Loss is \", loss.data.cpu().numpy().T)\n",
    "    \n",
    "    # optimization\n",
    "    grad_loss = torch.t(x_train).mm(h_1-y_train_expand)\n",
    "    hessian_loss = torch.zeros(D_in, D_in, num_classes, device=device, dtype=dtype)\n",
    "    dtheta_2 = torch.zeros(theta_2.size(), device=device, dtype=dtype)\n",
    "    for i in range(num_classes):\n",
    "        temp_diag = torch.diag(torch.flatten(h_0[:, i]*h_1[:, i]+tolerance))\n",
    "        hessian_loss[:, :, i] = torch.t(x_train).mm(temp_diag).mm(x_train)\n",
    "        temp = torch.inverse(hessian_loss[:, :, i]).mm(grad_loss[:, i].view(D_in, 1))\n",
    "        dtheta_2[:, i] = torch.flatten(torch.inverse(hessian_loss[:, :, i]).mm(grad_loss[:, i].view(D_in, 1)))\n",
    "    theta_2 -= step_size * dtheta_2 + reg*theta_2\n",
    "    \n",
    "    # backtracing line search related\n",
    "    pk = -dtheta_2\n",
    "    alpha = rho*alpha\n",
    "    criterion = (-log_likelyhood_mul_cal(x_train.mm(theta_2+alpha*pk), y_train_expand)/N \\\n",
    "                +log_likelyhood_mul_cal(h_1, y_train_expand)/N \\\n",
    "                -c*alpha*torch.diag(torch.t(grad_loss/N).mm(pk)).view(num_classes, 1))>0\n",
    "    criterion_v = torch.sum(criterion.int())\n",
    "    print(\"Criterion function value is: \", criterion_v.item())\n",
    "    \n",
    "print(\"\\nParameters theta_2 is:\\n\", theta_2.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.7661538461538462\n"
     ]
    }
   ],
   "source": [
    "# check accuracy for test data\n",
    "y_test_expand = torch.zeros(y_test.size(0), num_classes, device=device, dtype=dtype)\n",
    "# print(y_test_expand.size())\n",
    "for i in range(num_classes):\n",
    "    temp = y_test == i+1\n",
    "    y_test_expand[:, i] = torch.flatten(temp.int())\n",
    "\n",
    "y_test_cal = x_test.mm(theta_2)\n",
    "sigmoid_y = 1/(1+torch.exp(-y_test_cal)) > 0.5\n",
    "# print(sigmoid_y)\n",
    "accuracy = torch.true_divide(torch.sum(torch.abs(y_test_expand * sigmoid_y.int())), y_test_expand.size(0))\n",
    "print(\"Accuracy is \", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is  [[0.693 0.693 0.693 0.693]]\n",
      "Loss is  [[nan nan nan nan]]\n",
      "\n",
      "Parameters theta_2 is:\n",
      " [[-3.059e-01  2.145e+00 -4.240e+00 -7.666e+00]\n",
      " [-4.480e+00 -6.145e+00  2.567e+00  1.498e+00]\n",
      " [-3.564e+00 -6.218e+00  1.396e+00 -1.797e+00]\n",
      " [-4.527e-01 -8.818e-01 -2.205e+00 -1.268e+01]\n",
      " [-2.175e+00 -5.957e+00  1.323e+00 -2.583e+00]\n",
      " [-9.279e-01 -7.008e+00  3.157e-01 -8.477e+00]\n",
      " [-3.402e+00 -2.028e+00  2.539e+00 -1.160e+01]\n",
      " [-1.054e+00 -2.712e+00 -1.236e+00  4.146e-01]\n",
      " [-5.544e-01 -3.478e+00 -2.271e+00 -3.471e+00]\n",
      " [ 3.551e+00 -8.046e+00  3.536e+00 -7.168e+00]\n",
      " [ 5.922e+00  3.443e+00 -2.853e+00 -8.080e+00]\n",
      " [ 6.274e+00  1.035e+00 -1.918e+00 -1.060e+01]\n",
      " [ 3.136e+00 -4.012e+00  1.835e+00 -3.359e-01]\n",
      " [ 2.889e+00  4.632e+00 -1.459e+00 -7.173e-01]\n",
      " [ 2.602e+00  3.830e+00 -6.033e-01  6.397e-01]\n",
      " [ 5.593e+00 -1.856e+00 -2.944e+00  1.431e+00]\n",
      " [ 1.896e+00  1.217e+00  1.079e+00 -4.353e+00]\n",
      " [ 2.104e+00  6.456e-01  1.964e+00 -3.819e+00]\n",
      " [-6.528e-01  1.066e+00  1.099e-01  2.681e+00]\n",
      " [-4.626e-02  1.057e-01 -4.466e-02  1.463e-01]\n",
      " [-1.417e-01  2.402e-01 -2.843e-02  6.265e-01]\n",
      " [-5.612e-01  1.035e+00  1.614e-02  2.868e+00]\n",
      " [-4.827e-02  3.435e-02  1.108e-02  1.586e-01]\n",
      " [-1.408e-01  2.027e-01 -1.565e-02  6.890e-01]\n",
      " [-5.795e-01  1.014e+00 -5.102e-02  2.630e+00]\n",
      " [-4.516e-02  1.588e-01 -1.025e-01  2.401e-01]\n",
      " [-7.627e-02  2.769e-01 -1.089e-01  5.729e-01]]\n"
     ]
    }
   ],
   "source": [
    "i, step_size = 0, 1.0\n",
    "decay_rate = 0.8\n",
    "loss = torch.ones(num_classes, device=device, dtype=dtype)\n",
    "theta_2 = torch.zeros(D_in, D_out, device=device, dtype=dtype)\n",
    "while i < iteration and (torch.max(loss) > tolerance or torch.sum(torch.isinf(loss).int())):\n",
    "    i = i+1\n",
    "    step_size = step_size * decay_rate\n",
    "    # forward pass\n",
    "    scores = x_train.mm(theta_2)\n",
    "    # computing loss\n",
    "    h_1 = sigmoid(scores)\n",
    "    h_0 = 1 - h_1\n",
    "    log_likelyhood = log_likelyhood_mul_cal(h_1, y_train_expand)\n",
    "    loss = -log_likelyhood/N + 0.5*reg*torch.diag(torch.t(theta_2).mm(theta_2)).view(num_classes, 1)\n",
    "    print(\"Loss is \", loss.data.cpu().numpy().T)\n",
    "    \n",
    "    # optimization\n",
    "    grad_loss = torch.t(x_train).mm(h_1-y_train_expand)\n",
    "    theta_2 -= step_size * grad_loss/N + reg*theta_2\n",
    "    \n",
    "print(\"\\nParameters theta_2 is:\\n\", theta_2.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.3323076923076923\n"
     ]
    }
   ],
   "source": [
    "# check accuracy for test data\n",
    "y_test_expand = torch.zeros(y_test.size(0), num_classes, device=device, dtype=dtype)\n",
    "# print(y_test_expand.size())\n",
    "for i in range(num_classes):\n",
    "    temp = y_test == i+1\n",
    "    y_test_expand[:, i] = torch.flatten(temp.int())\n",
    "\n",
    "y_test_cal = x_test.mm(theta_2)\n",
    "sigmoid_y = 1/(1+torch.exp(-y_test_cal)) > 0.5\n",
    "# print(sigmoid_y)\n",
    "accuracy = torch.true_divide(torch.sum(torch.abs(y_test_expand * sigmoid_y.int())), y_test_expand.size(0))\n",
    "print(\"Accuracy is \", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5\n",
    "\n",
    "The gradient descent has less computation when updating the parameters, while extra parameter, like the decay rate should be set as it's likely that the results from gradient descent is ocsillating arount the optimal point and the step size should decrease during the iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for batch size of  1  is:  [[0.004 0.004 0.004 0.004]]\n",
      "Loss for batch size of  1  is:  [[4.942e-06       nan 4.942e-06 4.942e-06]]\n",
      "Accuracy for batch size of  1  is  0.7415384615384616 \n",
      "\n",
      "Loss for batch size of  16  is:  [[0.056 0.056 0.056 0.056]]\n",
      "Loss for batch size of  16  is:  [[2.369 2.556 1.278 2.487]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.398 1.178 0.412]]\n",
      "Loss for batch size of  16  is:  [[0.004 1.035 1.128 0.276]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.498 1.103 0.421]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.228 1.09  0.162]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.094 1.084 0.06 ]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.037 1.081 0.042]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.022 1.079 0.034]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.018 1.078 0.031]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.017 1.078 0.029]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.016 1.078 0.029]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.016 1.077 0.028]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.015 1.077 0.028]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.015 1.077 0.028]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.015 1.077 0.028]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.015 1.077 0.028]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.015 1.077 0.028]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.015 1.077 0.028]]\n",
      "Loss for batch size of  16  is:  [[0.003 0.015 1.077 0.028]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.086 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.085 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.085 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.084 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.084 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.083 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.082 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.082 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.081 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.081 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.08  0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.079 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.079 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.078 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.077 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.077 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.076 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.076 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.075 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.025 0.014 6.074 0.057]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.044 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.043 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.043 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.043 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.042 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.042 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.042 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.041 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.041 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.041 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.04  0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.04  0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.04  0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.04  0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.039 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.039 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.039 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.038 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.038 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.004 0.008 3.038 0.027]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.046 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.046 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.045 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.045 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.045 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.044 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.044 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.043 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.043 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.043 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.042 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.042 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.041 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.041 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.04  0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.04  0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.04  0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.039 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.039 0.086]]\n",
      "Loss for batch size of  16  is:  [[0.021 0.012 4.038 0.086]]\n",
      "Loss for batch size of  16  is:  [[3.032e-03 7.490e-03 4.322e+00 8.107e-02]]\n",
      "Loss for batch size of  16  is:  [[3.033e-03 7.491e-03 4.321e+00 8.106e-02]]\n",
      "Loss for batch size of  16  is:  [[3.033e-03 7.491e-03 4.321e+00 8.105e-02]]\n",
      "Loss for batch size of  16  is:  [[3.034e-03 7.492e-03 4.320e+00 8.104e-02]]\n",
      "Loss for batch size of  16  is:  [[3.035e-03 7.492e-03 4.320e+00 8.103e-02]]\n",
      "Loss for batch size of  16  is:  [[3.035e-03 7.492e-03 4.319e+00 8.103e-02]]\n",
      "Loss for batch size of  16  is:  [[3.036e-03 7.493e-03 4.319e+00 8.102e-02]]\n",
      "Loss for batch size of  16  is:  [[3.036e-03 7.493e-03 4.319e+00 8.101e-02]]\n",
      "Loss for batch size of  16  is:  [[3.037e-03 7.494e-03 4.318e+00 8.100e-02]]\n",
      "Loss for batch size of  16  is:  [[3.038e-03 7.494e-03 4.318e+00 8.100e-02]]\n",
      "Loss for batch size of  16  is:  [[3.038e-03 7.495e-03 4.317e+00 8.099e-02]]\n",
      "Loss for batch size of  16  is:  [[3.039e-03 7.495e-03 4.317e+00 8.098e-02]]\n",
      "Loss for batch size of  16  is:  [[3.040e-03 7.496e-03 4.316e+00 8.097e-02]]\n",
      "Loss for batch size of  16  is:  [[3.040e-03 7.496e-03 4.316e+00 8.096e-02]]\n",
      "Loss for batch size of  16  is:  [[3.041e-03 7.496e-03 4.316e+00 8.096e-02]]\n",
      "Loss for batch size of  16  is:  [[3.041e-03 7.497e-03 4.315e+00 8.095e-02]]\n",
      "Loss for batch size of  16  is:  [[3.042e-03 7.497e-03 4.315e+00 8.094e-02]]\n",
      "Loss for batch size of  16  is:  [[3.043e-03 7.498e-03 4.314e+00 8.093e-02]]\n",
      "Loss for batch size of  16  is:  [[3.043e-03 7.498e-03 4.314e+00 8.092e-02]]\n",
      "Loss for batch size of  16  is:  [[3.044e-03 7.499e-03 4.313e+00 8.092e-02]]\n",
      "Loss for batch size of  16  is:  [[7.484e-03 5.587e-03 6.181e+00 2.297e-02]]\n",
      "Loss for batch size of  16  is:  [[7.484e-03 5.587e-03 6.180e+00 2.296e-02]]\n",
      "Loss for batch size of  16  is:  [[7.485e-03 5.588e-03 6.180e+00 2.296e-02]]\n",
      "Loss for batch size of  16  is:  [[7.485e-03 5.588e-03 6.179e+00 2.296e-02]]\n",
      "Loss for batch size of  16  is:  [[7.485e-03 5.589e-03 6.178e+00 2.296e-02]]\n",
      "Loss for batch size of  16  is:  [[7.486e-03 5.589e-03 6.178e+00 2.296e-02]]\n",
      "Loss for batch size of  16  is:  [[7.486e-03 5.590e-03 6.177e+00 2.295e-02]]\n",
      "Loss for batch size of  16  is:  [[7.487e-03 5.590e-03 6.177e+00 2.295e-02]]\n",
      "Loss for batch size of  16  is:  [[7.487e-03 5.590e-03 6.176e+00 2.295e-02]]\n",
      "Loss for batch size of  16  is:  [[7.487e-03 5.591e-03 6.175e+00 2.295e-02]]\n",
      "Loss for batch size of  16  is:  [[7.488e-03 5.591e-03 6.175e+00 2.295e-02]]\n",
      "Loss for batch size of  16  is:  [[7.488e-03 5.592e-03 6.174e+00 2.295e-02]]\n",
      "Loss for batch size of  16  is:  [[7.489e-03 5.592e-03 6.173e+00 2.294e-02]]\n",
      "Loss for batch size of  16  is:  [[7.489e-03 5.593e-03 6.173e+00 2.294e-02]]\n",
      "Loss for batch size of  16  is:  [[7.489e-03 5.593e-03 6.172e+00 2.294e-02]]\n",
      "Loss for batch size of  16  is:  [[7.490e-03 5.594e-03 6.172e+00 2.294e-02]]\n",
      "Loss for batch size of  16  is:  [[7.490e-03 5.594e-03 6.171e+00 2.294e-02]]\n",
      "Loss for batch size of  16  is:  [[7.491e-03 5.595e-03 6.170e+00 2.294e-02]]\n",
      "Loss for batch size of  16  is:  [[7.491e-03 5.595e-03 6.170e+00 2.293e-02]]\n",
      "Loss for batch size of  16  is:  [[7.492e-03 5.596e-03 6.169e+00 2.293e-02]]\n",
      "Loss for batch size of  16  is:  [[0.012 0.02  5.065   nan]]\n",
      "Accuracy for batch size of  16  is  0.52 \n",
      "\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.112 0.112 0.112 0.112]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Loss for batch size of  32  is:  [[0.021 0.021 0.021 0.021]]\n",
      "Accuracy for batch size of  32  is  0.0 \n",
      "\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Loss for batch size of  198  is:  [[0.693 0.693 0.693 0.693]]\n",
      "Accuracy for batch size of  198  is  0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "step_size, decay_rate = 1.0, 0.5\n",
    "\n",
    "batch_sizes = [1, 16, 32, N]\n",
    "theta_s = torch.zeros(D_in, D_out, len(batch_sizes), device=device, dtype=dtype)\n",
    "\n",
    "y_test_expand = torch.zeros(y_test.size(0), num_classes, device=device, dtype=dtype)\n",
    "# print(y_test_expand.size())\n",
    "for i in range(num_classes):\n",
    "    temp = y_test == i+1\n",
    "    y_test_expand[:, i] = torch.flatten(temp.int())\n",
    "\n",
    "for k in range(len(batch_sizes)):\n",
    "    batch_size = batch_sizes[k]\n",
    "    loss = torch.ones(num_classes, device=device, dtype=dtype)\n",
    "    theta_2 = torch.zeros(D_in, D_out, device=device, dtype=dtype)\n",
    "    perm_seq = torch.randperm(N)\n",
    "    x_train_perm, y_train_expand_perm = x_train[perm_seq, :], y_train_expand[perm_seq, :]\n",
    "    \n",
    "    for j in range(int(np.ceil(N/batch_size))):\n",
    "        # print(\"j is \", j)\n",
    "        start_ind = batch_size*j\n",
    "        end_ind = np.minimum(N, start_ind+batch_size)\n",
    "        # print(\"end_ind is \",  end_ind)\n",
    "        x_batch = x_train_perm[start_ind:end_ind]\n",
    "        y_batch = y_train_expand_perm[start_ind:end_ind]\n",
    "        N_batch = end_ind - start_ind\n",
    "        \n",
    "        i = 0\n",
    "        while i < iteration and (torch.max(loss) > tolerance or torch.sum(torch.isinf(loss).int())):\n",
    "            i = i+1\n",
    "            step_size = step_size * decay_rate\n",
    "            # forward pass\n",
    "            scores = x_batch.mm(theta_2)\n",
    "            # computing loss\n",
    "            h_1 = sigmoid(scores)\n",
    "            h_0 = 1 - h_1\n",
    "            log_likelyhood = log_likelyhood_mul_cal(h_1, y_batch)\n",
    "            loss = -log_likelyhood/N + 0.5*reg*torch.diag(torch.t(theta_2).mm(theta_2)).view(num_classes, 1)\n",
    "            print(\"Loss for batch size of \", batch_size, \" is: \", loss.data.cpu().numpy().T)\n",
    "            # optimization\n",
    "            grad_loss = torch.t(x_batch).mm(h_1-y_batch)\n",
    "            theta_2 -= step_size * grad_loss/N + reg*theta_2\n",
    "    \n",
    "    theta_s[:, :, k] = theta_2\n",
    "    # print(\"\\nParameters theta_1 is:\\n\", theta_1.data.cpu().numpy())\n",
    "    y_test_cal = x_test.mm(theta_2)\n",
    "    sigmoid_y = sigmoid(y_test_cal) > 0.5\n",
    "    # print(sigmoid_y)\n",
    "    accuracy = torch.true_divide(torch.sum(torch.abs(y_test_binary * sigmoid_y.int())), y_test_binary.size(0))\n",
    "    print(\"Accuracy for batch size of \", batch_size, \" is \", accuracy.item(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_gpu]",
   "language": "python",
   "name": "conda-env-pytorch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
